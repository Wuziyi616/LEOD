reproduce:
  seed_everything: null # Union[int, null]
  deterministic_flag: False # Must be true for fully deterministic behaviour (slows down training)
  benchmark: True # Should be set to false for fully deterministic behaviour. Could potentially speed up training.
training:
  precision: 16
  max_epochs: 10000
  max_steps: 400000
  learning_rate: 0.0002
  weight_decay: 0
  gradient_clip_val: 1.0
  limit_train_batches: 1.0
  lr_scheduler:
    use: True
    total_steps: ${..max_steps}
    pct_start: 0.005
    div_factor: 25 # init_lr = max_lr / div_factor
    final_div_factor: 10000 # final_lr = max_lr / final_div_factor (this is different from Pytorch' OneCycleLR param)
validation:
  limit_val_batches: 1.0
  val_check_interval: 20000 # Optional[int]
  check_val_every_n_epoch: null # Optional[int]
batch_size:
  train: 8
  eval: 8
hardware:
  num_workers:
    train: 8
    eval: 8
  gpus: 0 # Either a single integer (e.g. 3) or a list of integers (e.g. [3,5,6])
  dist_backend: "nccl"
logging:
  ckpt_every_min: 18 # checkpoint every x minutes
  train:
    metrics:
      compute: false
      detection_metrics_every_n_steps: null # Optional[int] -> null: every train epoch, int: every N steps
    log_model_every_n_steps: 5000
    log_every_n_steps: 100
    high_dim:
      enable: True
      every_n_steps: 5000
      n_samples: 4
  validation:
    high_dim:
      enable: True
      every_n_epochs: 1
      n_samples: 8
wandb:
  wandb_name: null # name of the run
  wandb_id: null # name of the run
  wandb_runpath: null # WandB run path. E.g. USERNAME/PROJECTNAME/1grv5kg6
  group_name: null # Specify group name of the run
  project_name: RVT
suffix: "" # full dup run name suffix
weight: "" # only resume weight
checkpoint: "" # resume weight + training state
pretrain_teacher_checkpoint: ""  # pre-trained weight of the teacher model
pretrain_student_checkpoint: ""  # pre-trained weight of the student model
